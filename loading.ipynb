{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import simplejson as json\n",
    "def load_data_file(filepath):\n",
    "    \"\"\"Load the json file, and check the version.\"\"\"\n",
    "    with open(filepath) as data_file:\n",
    "        parsed_file = json.load(data_file)\n",
    "        if (parsed_file['version'] != '1.0'):\n",
    "            raise ValueError('Dataset version unrecognized.')\n",
    "        return parsed_file['data']\n",
    "    \n",
    "train, dev = load_data_file('./data/train-v1.0.json'), load_data_file('./data/dev-v1.0.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [04:31<00:00,  1.57it/s]\n",
      "100%|██████████| 48/48 [00:36<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def transform_data_annotate(data):\n",
    "    articles = []\n",
    "    for article in tqdm(data):\n",
    "        paragraphs = []\n",
    "        for paragraph in article['paragraphs']:\n",
    "            qas = []\n",
    "            for qa in paragraph['qas']:\n",
    "                answers = []\n",
    "                for answer in qa['answers']:\n",
    "                    answers.append({\n",
    "                        'text': nlp(answer['text'], entity = True),\n",
    "                        'answer_start': answer['answer_start'],\n",
    "                    })\n",
    "                qas.append({\n",
    "                        'question': nlp(qa['question'], entity = True),\n",
    "                        'answers': answers\n",
    "                    })\n",
    "            paragraphs.append({\n",
    "                    'context': nlp(paragraph['context'], entity = True),\n",
    "                    'qas': qas\n",
    "                })\n",
    "        articles.append({\n",
    "                'title': nlp(article['title'], entity = True),\n",
    "                'paragraphs': paragraphs\n",
    "            })\n",
    "    return articles\n",
    "\n",
    "train_annotated, dev_annotated = transform_data_annotate(train), transform_data_annotate(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make answers spans of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:42<00:00,  9.44it/s]\n",
      "100%|██████████| 48/48 [00:12<00:00,  3.37it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_span_contain_position(s1, s2):\n",
    "    m = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n",
    "    longest, x_longest = 0, 0\n",
    "    for x in range(1, 1 + len(s1)):\n",
    "        for y in range(1, 1 + len(s2)):\n",
    "            if s1[x - 1] == s2[y - 1]:\n",
    "                m[x][y] = m[x - 1][y - 1] + 1\n",
    "                if m[x][y] > longest:\n",
    "                    longest = m[x][y]\n",
    "                    x_longest = x\n",
    "            else:\n",
    "                m[x][y] = 0\n",
    "    return x_longest - longest, x_longest\n",
    "\n",
    "\n",
    "def transform_data_answer_spannify(data):\n",
    "    articles = []\n",
    "    for article in tqdm(data):\n",
    "        paragraphs = []\n",
    "        for paragraph in article['paragraphs']:\n",
    "            qas = []\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                answers = []\n",
    "                for answer in qa['answers']:\n",
    "                    context_text = [x.text for x in context]\n",
    "                    answer_text = [x.text for x in answer['text']]\n",
    "                    start_index, end_index = get_span_contain_position(context_text, answer_text)\n",
    "                    answer = context[start_index:end_index]\n",
    "                    answers.append(answer)\n",
    "                qa['answers'] = answers\n",
    "                qas.append(qa)\n",
    "            paragraph['qas'] = qas\n",
    "            paragraphs.append(paragraph)\n",
    "        article['paragraphs'] = paragraphs\n",
    "        articles.append(article)\n",
    "    return articles\n",
    "\n",
    "train_spanned, dev_spanned = transform_data_answer_spannify(train_annotated), transform_data_answer_spannify(dev_annotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put input in right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:05<00:00, 74.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3706 25651 239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_max_lengths(data):\n",
    "    max_context = max_question = max_answer = 0\n",
    "\n",
    "    def set_longest(text, max_field):\n",
    "        if len(text.text) > max_field:\n",
    "            max_field = len(text.text)\n",
    "        return max_field\n",
    "\n",
    "    for article in tqdm(data):\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            max_context = set_longest(context, max_context)\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                max_question = set_longest(question, max_question)\n",
    "                for answer in qa['answers']:\n",
    "                    max_answer = set_longest(answer, max_answer)\n",
    "\n",
    "    return max_context, max_question, max_answer\n",
    "\n",
    "max_context, max_question, max_answer = get_max_lengths(train_spanned)\n",
    "print(max_context, max_question, max_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:14<00:00, 29.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "def transform_data_to_model_format(data, max_context, max_question, max_answer):\n",
    "    store = spacy.strings.StringStore()\n",
    "    contexts, questions, answers = [], [], []\n",
    "    \n",
    "    for article in tqdm(data):\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            c = np.zeros(max_context, dtype=np.int)\n",
    "            c[:len(context)] = [store[token.orth_] for token in context]\n",
    "            sparse.csr_matrix(\n",
    ".            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                q = np.zeros(max_question, dtype=np.int)\n",
    "                q[:len(question)] = [store[token.orth_] for token in question]\n",
    "                for answer in qa['answers']:\n",
    "                    a = np.zeros((max_context, 2), dtype=np.int) # not max_answer\n",
    "                    \n",
    "                    a[answer.start: answer.end, 1] = 1\n",
    "                    a[:answer.start, 0] = 1\n",
    "                    a[answer.end:  , 0] = 1\n",
    "                    \n",
    "                    contexts.append(c)\n",
    "                    questions.append(q)\n",
    "                    answers.append(a)\n",
    "    return sparse.csr_matrix(contexts), sparse.csr_matrix(questions), sparse.csr_matrix(answers), len(store)\n",
    "\n",
    "train_contexts, train_questions, train_answers, vocab_size = transform_data_to_model_format(train_spanned, max_context, max_question, max_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created save object\n"
     ]
    }
   ],
   "source": [
    "save_obj = {\n",
    "    'contexts': train_contexts,\n",
    "    'questions': train_questions,\n",
    "    'answers': train_answers,\n",
    "    'vocab_size': vocab_size\n",
    "}\n",
    "\n",
    "print(\"Created save object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train.dat', 'wb') as outfile:\n",
    "    pickle.dump(save_obj, outfile, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
