{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD - A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy loaded.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load spacy.\"\"\"\n",
    "from spacy.en import English\n",
    "spacy = English()\n",
    "print('Spacy loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import simplejson as json\n",
    "def load_data_file(filepath):\n",
    "    \"\"\"Load the json file, and check the version.\"\"\"\n",
    "    with open(filepath) as data_file:\n",
    "        parsed_file = json.load(data_file)\n",
    "        if (parsed_file['version'] != '1.0'):\n",
    "            raise ValueError('Dataset version unrecognized.')\n",
    "        return parsed_file['data']\n",
    "    \n",
    "train, dev = load_data_file('./data/train-v1.0.json'), load_data_file('./data/dev-v1.0.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap between train and dev contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:01<00:00, 417.97it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 365.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique Context Words in train: 169632\n",
      "# Unique Context Words in dev: 38449\n",
      "# Unique Context Words in dev not in train: 12572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def simple_tokenize(text):\n",
    "        return [word.lower() for word in text.split()]\n",
    "\n",
    "def get_context_vocab(data):    \n",
    "    \"\"\"Get the set of words in the paragraphs of data.\"\"\"\n",
    "    set_of_words_in_data = set()\n",
    "    for article in tqdm(data):\n",
    "        for paragraph in article['paragraphs']:\n",
    "            c = paragraph['context']\n",
    "            c_tokens = simple_tokenize(c)\n",
    "            set_of_words_in_data |= set(c_tokens)\n",
    "    return set_of_words_in_data\n",
    "\n",
    "train_context_words = get_context_vocab(train)\n",
    "print(\"# Unique Context Words in train:\", len(train_context_words))\n",
    "dev_words = get_context_vocab(dev)\n",
    "print(\"# Unique Context Words in dev:\", len(dev_words))\n",
    "difference_words = dev_words - train_context_words\n",
    "print(\"# Unique Context Words in dev not in train:\", len(difference_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap between train context and dev answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique Context Words in train: 169632\n",
      "# Unique Context Words in train: 169632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 325.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique Answer Words in dev: 13002\n",
      "# Unique Answer Words in dev but not in context train: 3249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_answer_vocab(data):\n",
    "    \"\"\"Get the set of words in the answers of data.\"\"\"\n",
    "    set_of_words_in_data = set()\n",
    "    for article in tqdm(data):\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                answer = qa['answers'][0]\n",
    "                for answer in qa['answers']:\n",
    "                    a = answer['text']\n",
    "                    a_tokens = simple_tokenize(a)\n",
    "                    tok_set = set(a_tokens)\n",
    "                    set_of_words_in_data |= tok_set\n",
    "    return set_of_words_in_data\n",
    "\n",
    "print(\"# Unique Context Words in train:\", len(train_context_words))\n",
    "print(\"# Unique Context Words in train:\", len(train_context_words))\n",
    "dev_answer_words = get_answer_vocab(dev)\n",
    "print(\"# Unique Answer Words in dev:\", len(dev_answer_words))\n",
    "difference_words = dev_answer_words - train_context_words\n",
    "print(\"# Unique Answer Words in dev but not in context train:\",\n",
    "      len(difference_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you annotate data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [04:54<00:00,  1.61it/s]\n",
      "100%|██████████| 48/48 [00:37<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "def transform_data_annotate(data):\n",
    "    articles = []\n",
    "    for article in tqdm(data):\n",
    "        paragraphs = []\n",
    "        for paragraph in article['paragraphs']:\n",
    "            qas = []\n",
    "            for qa in paragraph['qas']:\n",
    "                answers = []\n",
    "                for answer in qa['answers']:\n",
    "                    answers.append({\n",
    "                        'text': spacy(answer['text']),\n",
    "                        'answer_start': answer['answer_start'],\n",
    "                    })\n",
    "                qas.append({\n",
    "                        'question': spacy(qa['question']),\n",
    "                        'answers': answers\n",
    "                    })\n",
    "            paragraphs.append({\n",
    "                    'context': spacy(paragraph['context']),\n",
    "                    'qas': qas\n",
    "                })\n",
    "        articles.append({\n",
    "                'title': spacy(article['title']),\n",
    "                'paragraphs': paragraphs\n",
    "            })\n",
    "    return articles\n",
    "\n",
    "train_annotated, dev_annotated = transform_data_annotate(train), transform_data_annotate(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make answers spans of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:34<00:00, 12.70it/s]\n",
      "100%|██████████| 48/48 [00:13<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_span_contain_position(s1, s2):\n",
    "    m = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n",
    "    longest, x_longest = 0, 0\n",
    "    for x in range(1, 1 + len(s1)):\n",
    "        for y in range(1, 1 + len(s2)):\n",
    "            if s1[x - 1] == s2[y - 1]:\n",
    "                m[x][y] = m[x - 1][y - 1] + 1\n",
    "                if m[x][y] > longest:\n",
    "                    longest = m[x][y]\n",
    "                    x_longest = x\n",
    "            else:\n",
    "                m[x][y] = 0\n",
    "    return x_longest - longest, x_longest\n",
    "\n",
    "\n",
    "def transform_data_answer_spannify(data):\n",
    "    articles = []\n",
    "    for article in tqdm(data):\n",
    "        paragraphs = []\n",
    "        for paragraph in article['paragraphs']:\n",
    "            qas = []\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                answers = []\n",
    "                for answer in qa['answers']:\n",
    "                    context_text = list(map(lambda x: x.text, context))\n",
    "                    answer_text = list(map(lambda x: x.text, answer['text']))\n",
    "                    start_index, end_index = get_span_contain_position(context_text, answer_text)\n",
    "                    answer = context[start_index:end_index]\n",
    "                    answers.append(answer)\n",
    "                qa['answers'] = answers\n",
    "                qas.append(qa)\n",
    "            paragraph['qas'] = qas\n",
    "            paragraphs.append(paragraph)\n",
    "        article['paragraphs'] = paragraphs\n",
    "        articles.append(article)\n",
    "    return articles\n",
    "\n",
    "train_spanned, dev_spanned = transform_data_answer_spannify(train_annotated), transform_data_answer_spannify(dev_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_data_into_triples(data):\n",
    "    for article in data:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    yield context, question, answer\n",
    "\n",
    "train_triples, dev_triples = list(transform_data_into_triples(train_spanned)), list(transform_data_into_triples(dev_spanned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put input in right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33615/33615 [00:02<00:00, 16427.39it/s]\n",
      " 93%|█████████▎| 31380/33615 [00:01<00:00, 26016.19it/s]"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def transform_to_model_format(triples):\n",
    "    def get_max_lengths(triples):\n",
    "        max_context = max_question = max_answer = 0\n",
    "        def set_longest(text, max_field):\n",
    "            if len(text.text) > max_field:\n",
    "                max_field = len(text.text)\n",
    "            return max_field\n",
    "\n",
    "        for context, question, answer in tqdm(triples):\n",
    "            max_context = set_longest(context, max_context)\n",
    "            max_question = set_longest(question, max_question)\n",
    "            max_answer = set_longest(answer, max_answer)\n",
    "\n",
    "        return max_context, max_question, max_answer\n",
    "\n",
    "    max_context, max_question, max_answer = get_max_lengths(triples)\n",
    "    \n",
    "    def preprocess_triples(triples):\n",
    "        for context, question, answer in tqdm(triples):\n",
    "            context = [token.orth for token in context]\n",
    "            question = [token.orth for token in question]\n",
    "            answer_start, answer_end = answer.start, answer.end\n",
    "            answer = np.zeros(max_answer)\n",
    "            answer[answer_start: answer_end] = 1\n",
    "            yield context, question, answer\n",
    "\n",
    "    contexts, questions, answers = zip(*preprocess_triples(triples))\n",
    "    contexts = pad_sequences(contexts, maxlen=max_context)\n",
    "    questions = pad_sequences(questions, maxlen=max_question)\n",
    "    return contexts, questions, answers\n",
    "\n",
    "print(transform_to_model_format(dev_triples)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GT 650M (CNMeM is enabled with initial size: 70.0% of memory, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, merge\n",
    "from keras.models import Model\n",
    "\n",
    "# headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n",
    "# note that we can name any layer by passing it a \"name\" argument.\n",
    "main_input = Input(shape=(100,), dtype='int32', name='main_input')\n",
    "\n",
    "# this embedding layer will encode the input sequence\n",
    "# into a sequence of dense 512-dimensional vectors.\n",
    "x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n",
    "\n",
    "# a LSTM will transform the vector sequence into a single vector,\n",
    "# containing information about the entire sequence\n",
    "lstm_out = LSTM(32)(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
