{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD - A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Load spacy.\"\"\"\n",
    "from spacy.en import English\n",
    "spacy = English()\n",
    "print('Spacy loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import simplejson as json\n",
    "def load_data_file(filepath):\n",
    "    \"\"\"Load the json file, and check the version.\"\"\"\n",
    "    with open(filepath) as data_file:\n",
    "        parsed_file = json.load(data_file)\n",
    "        if (parsed_file['version'] != '1.0'):\n",
    "            raise ValueError('Dataset version unrecognized.')\n",
    "        return parsed_file['data']\n",
    "    \n",
    "train, dev = load_data_file('./data/train-v1.0.json'), load_data_file('./data/dev-v1.0.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap between train and dev contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def simple_tokenize(text):\n",
    "        return [word.lower() for word in text.split()]\n",
    "\n",
    "def get_context_vocab(data):    \n",
    "    \"\"\"Get the set of words in the paragraphs of data.\"\"\"\n",
    "    set_of_words_in_data = set()\n",
    "    for article in tqdm(data):\n",
    "        for paragraph in article['paragraphs']:\n",
    "            c = paragraph['context']\n",
    "            c_tokens = simple_tokenize(c)\n",
    "            set_of_words_in_data |= set(c_tokens)\n",
    "    return set_of_words_in_data\n",
    "\n",
    "train_context_words = get_context_vocab(train)\n",
    "print(\"# Unique Context Words in train:\", len(train_context_words))\n",
    "dev_words = get_context_vocab(dev)\n",
    "print(\"# Unique Context Words in dev:\", len(dev_words))\n",
    "difference_words = dev_words - train_context_words\n",
    "print(\"# Unique Context Words in dev not in train:\", len(difference_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap between train context and dev answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_answer_vocab(data):\n",
    "    \"\"\"Get the set of words in the answers of data.\"\"\"\n",
    "    set_of_words_in_data = set()\n",
    "    for article in tqdm(data):\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                answer = qa['answers'][0]\n",
    "                for answer in qa['answers']:\n",
    "                    a = answer['text']\n",
    "                    a_tokens = simple_tokenize(a)\n",
    "                    tok_set = set(a_tokens)\n",
    "                    set_of_words_in_data |= tok_set\n",
    "    return set_of_words_in_data\n",
    "\n",
    "print(\"# Unique Context Words in train:\", len(train_context_words))\n",
    "print(\"# Unique Context Words in train:\", len(train_context_words))\n",
    "dev_answer_words = get_answer_vocab(dev)\n",
    "print(\"# Unique Answer Words in dev:\", len(dev_answer_words))\n",
    "difference_words = dev_answer_words - train_context_words\n",
    "print(\"# Unique Answer Words in dev but not in context train:\",\n",
    "      len(difference_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you annotate data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def annotate(data):\n",
    "    articles = []\n",
    "    for article in tqdm(data):\n",
    "        paragraphs = []\n",
    "        for paragraph in article['paragraphs']:\n",
    "            qas = []\n",
    "            for qa in paragraph['qas']:\n",
    "                answers = []\n",
    "                for answer in qa['answers']:\n",
    "                    answers.append({\n",
    "                        'text': spacy(answer['text']),\n",
    "                        'answer_start': answer['answer_start'],\n",
    "                    })\n",
    "                qas.append({\n",
    "                        'question': spacy(qa['question']),\n",
    "                        'answers': answers\n",
    "                    })\n",
    "            paragraphs.append({\n",
    "                    'context': spacy(paragraph['context']),\n",
    "                    'qas': qas\n",
    "                })\n",
    "        articles.append({\n",
    "                'title': spacy(article['title']),\n",
    "                'paragraphs': paragraphs\n",
    "            })\n",
    "    return articles\n",
    "\n",
    "train_annotated, dev_annotated = annotate(train), annotate(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_span_contain_position(s1, s2):\n",
    "    m = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n",
    "    longest, x_longest = 0, 0\n",
    "    for x in range(1, 1 + len(s1)):\n",
    "        for y in range(1, 1 + len(s2)):\n",
    "            if s1[x - 1] == s2[y - 1]:\n",
    "                m[x][y] = m[x - 1][y - 1] + 1\n",
    "                if m[x][y] > longest:\n",
    "                    longest = m[x][y]\n",
    "                    x_longest = x\n",
    "            else:\n",
    "                m[x][y] = 0\n",
    "    return x_longest - longest, x_longest\n",
    "\n",
    "import random\n",
    "sample_article = random.choice(dev_annotated)\n",
    "sample_paragraph = sample_article['paragraphs'][0]\n",
    "sample_answer = sample_paragraph['qas'][0]['answers'][0]\n",
    "\n",
    "sample_paragraph_text = sample_paragraph['context']\n",
    "sample_question_text = sample_paragraph['qas'][0]['question']\n",
    "sample_answer_text = sample_answer['text']\n",
    "sample_answer_start = sample_answer['answer_start']\n",
    "\n",
    "print(sample_paragraph_text, sample_question_text, sample_answer_text, sample_answer_start)\n",
    "start_index, end_index = get_span_contain_position(\n",
    "    list(map(lambda x: x.text, sample_paragraph_text)),\n",
    "    list(map(lambda x: x.text, sample_answer_text)))\n",
    "print(start_index, end_index)\n",
    "sample_paragraph_text[start_index:end_index].text == sample_answer_text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_answers_as_context_spans(data):\n",
    "    articles = []\n",
    "    for article in tqdm(data):\n",
    "        paragraphs = []\n",
    "        for paragraph in article['paragraphs']:\n",
    "            qas = []\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                answers = []\n",
    "                for answer in qa['answers']:\n",
    "                    context_text = list(map(lambda x: x.text, context))\n",
    "                    answer_text = list(map(lambda x: x.text, answer['text']))\n",
    "                    start_index, end_index = get_span_contain_position(context_text, answer_text)\n",
    "                    answer = context[start_index:end_index]\n",
    "                    answers.append(answer)\n",
    "                qa['answers'] = answers\n",
    "                qas.append(qa)\n",
    "            paragraph['qas'] = qas\n",
    "            paragraphs.append(paragraph)\n",
    "        article['paragraphs'] = paragraphs\n",
    "        articles.append(article)\n",
    "    return articles\n",
    "\n",
    "devlet = get_answers_as_context_spans(dev_annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_sample(data):\n",
    "    for article in data:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    yield context, question, answer\n",
    "\n",
    "next(print_sample(devlet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
